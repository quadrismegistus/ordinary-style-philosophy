{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3688a93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from osp import *\n",
    "\n",
    "df_meta = get_corpus_metadata()\n",
    "worddb = get_worddb().fillna('')\n",
    "ok_words = {word for word in worddb.index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e331dd53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecd8bafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>LMDBHashStash</pre><table border=\"1\" class=\"dataframe\"><thead><tr><th>Config</th><th>Param</th><th>Value</th></tr></thead><tbody><tr><td><b>Path</b></td><td>Root Dir</td><td><i>/Users/ryan/.cache/hashstash/osp_slices_1000</i></td></tr><tr><td><b></b></td><td>Filename</td><td><i>data.db</i></td></tr><tr><td><b>Engine</b></td><td>Engine</td><td><i>lmdb</i></td></tr><tr><td><b></b></td><td>Serializer</td><td><i>hashstash</i></td></tr><tr><td><b></b></td><td>Compress</td><td><i>lz4</i></td></tr><tr><td><b></b></td><td>B64</td><td><i>True</i></td></tr><tr><td><b>Stats</b></td><td>Len</td><td><i>57620</i></td></tr></tbody></table>"
      ],
      "text/plain": [
       "LMDBHashStash(~/.cache/hashstash/osp_slices_1000/lmdb.hashstash.lz4+b64/data.db)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STASH_SLICES = HashStash('osp_slices_1000')\n",
    "STASH_SLICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1e1567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "619d4931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', ' ', 'world', '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_agnostic(txt: str):\n",
    "    \"\"\"Tokenize text in a language-agnostic way.\n",
    "\n",
    "    Args:\n",
    "        txt: The input text.\n",
    "\n",
    "    Returns:\n",
    "        A list of tokens.\n",
    "    \"\"\"\n",
    "    return re.findall(r\"[\\w']+|[.,!?; -—–'\\n]\", txt)\n",
    "\n",
    "tokenize_agnostic(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decaa750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def iter_corpus_txt_slices(slice_len=10):\n",
    "#     num_recog = 0\n",
    "#     slicex = []\n",
    "#     for id, txt in iter_corpus_txt():\n",
    "#         num_slices = 0\n",
    "#         for si,sent in enumerate(txt.strip().split('\\n')):\n",
    "#             for word in tokenize_agnostic(sent):\n",
    "#                 if word.strip() and word.lower() in ok_words:\n",
    "#                     num_recog += 1\n",
    "#                 slicex.append(word)\n",
    "#             if num_recog >= slice_len:\n",
    "#                 num_slices += 1\n",
    "#                 otxt = ''.join(slicex).strip()\n",
    "#                 yield id,num_slices,otxt\n",
    "#                 slicex = []\n",
    "#                 num_recog = 0\n",
    "#             slicex.append('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88ebfc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "OK_WORDS = None\n",
    "def get_ok_words():\n",
    "    global OK_WORDS\n",
    "    if OK_WORDS is None:\n",
    "        worddb = get_worddb().fillna('')\n",
    "        OK_WORDS = {word for word in worddb.index}\n",
    "    return OK_WORDS\n",
    "\n",
    "\n",
    "def iter_txt_slices(txt, slice_len=10, ok_words=None):\n",
    "    num_recog = 0\n",
    "    slicex = []\n",
    "    num_slices = 0\n",
    "    for si,sent in enumerate(txt.strip().split('\\n')):\n",
    "        for word in tokenize_agnostic(sent):\n",
    "            if word.strip().isalpha() and word.lower() in ok_words:\n",
    "                num_recog += 1\n",
    "            slicex.append(word)\n",
    "        if num_recog >= slice_len:\n",
    "            num_slices += 1\n",
    "            otxt = ''.join(slicex).strip()\n",
    "            yield num_slices,otxt\n",
    "            slicex = []\n",
    "            num_recog = 0\n",
    "        slicex.append('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fce5842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_slices(id, force=False, slice_len=1000):\n",
    "    stash = HashStash(f'osp_slices_{slice_len}')\n",
    "    if not force and id in stash:\n",
    "        return stash[id]\n",
    "    txt = get_corpus_txt(id)\n",
    "    slices = dict(iter_txt_slices(txt, slice_len, get_ok_words()))\n",
    "    stash[id] = slices\n",
    "    return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e53ef71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_text_slices('phil/10.2307/40231690')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7161804b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57620/57620 [06:15<00:00, 153.37it/s]\n"
     ]
    }
   ],
   "source": [
    "for id in tqdm(df_meta.index):\n",
    "    get_text_slices(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "eb2a5a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>LMDBHashStash</pre><table border=\"1\" class=\"dataframe\"><thead><tr><th>Config</th><th>Param</th><th>Value</th></tr></thead><tbody><tr><td><b>Path</b></td><td>Root Dir</td><td><i>/Users/rj416/.cache/hashstash/osp_slices</i></td></tr><tr><td><b></b></td><td>Dbname</td><td><i>xxx</i></td></tr><tr><td><b></b></td><td>Filename</td><td><i>lmdb.hashstash.lz4+b64</i></td></tr><tr><td><b>Engine</b></td><td>Engine</td><td><i>lmdb</i></td></tr><tr><td><b></b></td><td>Serializer</td><td><i>hashstash</i></td></tr><tr><td><b></b></td><td>Compress</td><td><i>lz4</i></td></tr><tr><td><b></b></td><td>B64</td><td><i>True</i></td></tr></tbody></table>"
      ],
      "text/plain": [
       "LMDBHashStash(~/.cache/hashstash/osp_slices/lmdb.hashstash.lz4+b64)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5b88a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "21ded357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rj416/.cache/hashstash/osp_slices/lmdb.hashstash.lz4+b64/data.db'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STASH_SLICES.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a70289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
