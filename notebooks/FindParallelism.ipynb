{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a5099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from osp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c186145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id,docstr in JSTOR_STASH.items():\n",
    "    doc = stanza.Document.from_serialized(docstr)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42dc4213",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc.sentences:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a68a6dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  [\n",
       "    {\n",
       "      \"id\": 1,\n",
       "      \"text\": \"Hello\",\n",
       "      \"lemma\": \"hello\",\n",
       "      \"upos\": \"INTJ\",\n",
       "      \"xpos\": \"UH\",\n",
       "      \"head\": 0,\n",
       "      \"deprel\": \"root\",\n",
       "      \"start_char\": 0,\n",
       "      \"end_char\": 5,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ],\n",
       "      \"misc\": \"SpaceAfter=No\"\n",
       "    }\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nlp()(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c99b0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent(sent):\n",
    "    if isinstance(sent, str):\n",
    "        return get_nlp()(sent).sentences[0]\n",
    "    return sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67436767",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = get_sent(\"Loose maxims and corruptive principles haunt us.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b618c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok=sent.tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cb3a1760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'text': 'Loose',\n",
       " 'lemma': 'Loose',\n",
       " 'upos': 'ADJ',\n",
       " 'xpos': 'JJ',\n",
       " 'feats': 'Degree=Pos',\n",
       " 'head': 2,\n",
       " 'deprel': 'amod',\n",
       " 'start_char': 0,\n",
       " 'end_char': 5,\n",
       " 'ner': 'O',\n",
       " 'multi_ner': ('O',)}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.to_dict()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298cb485",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6d7fef08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(tokens):\n",
    "    l = []\n",
    "    for tok in tokens:\n",
    "        l.append(tok.text)\n",
    "        l.append(tok.spaces_after)\n",
    "    return ''.join(l).strip()\n",
    "\n",
    "def get_punct_tok(tok):\n",
    "    try:\n",
    "        return tok.to_dict()[0]['upos']\n",
    "    except Exception as e:\n",
    "        return ''\n",
    "\n",
    "def is_punct_tok(tok):\n",
    "    return get_punct_tok(tok) == 'PUNCT'\n",
    "\n",
    "def find_phrase_window(sent, tok_i, n, before=True):\n",
    "    phrase = []\n",
    "    phrase_nopunct = []\n",
    "    if before:\n",
    "        for tok in reversed(sent.tokens[:tok_i]):\n",
    "            phrase.insert(0, tok)\n",
    "            if not is_punct_tok(tok):\n",
    "                phrase_nopunct.insert(0, tok)\n",
    "            if len(phrase_nopunct) == n:\n",
    "                break\n",
    "    else:\n",
    "        for tok in sent.tokens[tok_i+1:]:\n",
    "            phrase.append(tok)\n",
    "            if not is_punct_tok(tok):\n",
    "                phrase_nopunct.append(tok)\n",
    "            if len(phrase_nopunct) == n:\n",
    "                break\n",
    "    return phrase, phrase_nopunct\n",
    "\n",
    "\n",
    "\n",
    "def find_parallelism(sent, max_n=10, center_pos = {'SCONJ', 'CCONJ', 'ADP'}):\n",
    "    sent = get_sent(sent)\n",
    "    ld = []\n",
    "    for tok_i, tok in enumerate(sent.tokens):\n",
    "        # if is_punct_tok(tok):\n",
    "        #     continue\n",
    "        if get_punct_tok(tok) not in center_pos:\n",
    "            continue\n",
    "        for n in range(2, max_n+1):\n",
    "            before_phrase, before_phrase_nopunct = find_phrase_window(sent, tok_i, n, before=True)\n",
    "            after_phrase, after_phrase_nopunct = find_phrase_window(sent, tok_i, n, before=False)\n",
    "\n",
    "            if len(before_phrase_nopunct) != n or len(after_phrase_nopunct) != n:\n",
    "                continue\n",
    "\n",
    "            before_phrase_pos = [get_punct_tok(tok) for tok in before_phrase_nopunct]\n",
    "            after_phrase_pos = [get_punct_tok(tok) for tok in after_phrase_nopunct]\n",
    "\n",
    "            if before_phrase_pos != after_phrase_pos:\n",
    "                continue\n",
    "\n",
    "            phrase = before_phrase + [tok] + after_phrase\n",
    "\n",
    "    \n",
    "            d = {\n",
    "                'phrase': detokenize(phrase),\n",
    "                'phrase_pos': ' '.join([get_punct_tok(tok) for tok in phrase]),\n",
    "                'part_pos': ' '.join(before_phrase_pos),\n",
    "                'center_pos': get_punct_tok(tok),\n",
    "                'phrase1': detokenize(before_phrase),\n",
    "                'phrase2': tok.text,\n",
    "                'phrase3': detokenize(after_phrase),\n",
    "            }\n",
    "            ld.append(d)\n",
    "    return ld\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "72f61bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = get_sent(\"Loose maxims, and, corruptive principles, haunt us.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "74130d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'phrase': 'Loose maxims, and, corruptive principles',\n",
       "  'phrase_pos': 'ADJ NOUN PUNCT CCONJ PUNCT ADJ NOUN',\n",
       "  'part_pos': 'ADJ NOUN',\n",
       "  'center_pos': 'CCONJ',\n",
       "  'phrase1': 'Loose maxims,',\n",
       "  'phrase2': 'and',\n",
       "  'phrase3': ', corruptive principles'}]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_parallelism(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9ce40842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parallelism_in_doc(doc):\n",
    "    ld = []\n",
    "    for sent in doc.sentences:\n",
    "        for d in find_parallelism(sent):\n",
    "            ld.append({'sent':detokenize(sent.tokens), **d})\n",
    "    return ld\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "99e4c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "STASH_PARALLELISM = HashStash('osp_parallel')\n",
    "\n",
    "def find_parallelism_in_stash(stash, force=False):\n",
    "    ld = []\n",
    "    for id,docstr in tqdm(stash.items(), total=len(stash)):\n",
    "        if not force and id in STASH_PARALLELISM:\n",
    "            doc_ld = STASH_PARALLELISM[id]\n",
    "        else:\n",
    "            doc = stanza.Document.from_serialized(docstr)\n",
    "            doc_ld = find_parallelism_in_doc(doc)\n",
    "            STASH_PARALLELISM[id] = doc_ld\n",
    "        \n",
    "        for d in doc_ld:\n",
    "            ld.append({'id':id, **d})\n",
    "    return ld\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "068642e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(STASH_PARALLELISM.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e19a393f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8840/8840 [1:36:36<00:00,  1.53it/s]  \n"
     ]
    }
   ],
   "source": [
    "ld_pmla = find_parallelism_in_stash(PMLA_STASH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "221ce0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16859/16859 [3:35:25<00:00,  1.30it/s]  \n"
     ]
    }
   ],
   "source": [
    "ld_jstor = find_parallelism_in_stash(JSTOR_STASH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec69a02c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb06d22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
